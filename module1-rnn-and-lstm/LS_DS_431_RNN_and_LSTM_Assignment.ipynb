{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "U4-S3-DNN (Python 3.7)",
      "language": "python",
      "name": "u4-s3-dnn"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "LS_DS_431_RNN_and_LSTM_Assignment.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU6OSdCpKX2Z",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
        "\n",
        "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
        "\n",
        "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
        "\n",
        "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
        "\n",
        "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
        "\n",
        "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
        "\n",
        "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
        "\n",
        "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ltj1je1fp5rO",
        "colab": {}
      },
      "source": [
        "# I'm gonna import with pandas - it'll be good practice :)\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_fwf(\"https://www.gutenberg.org/files/100/100-0.txt\").reset_index()\n",
        "\n",
        "df = df.drop('Unnamed: 0', axis=1)  # Dropping the unnamed column\n",
        "df = df.dropna().reset_index(drop=True)  # Dropping NaN values, resetting index.\n",
        "\n",
        "df = df[63:138996].reset_index(drop=True)  # This contains the actual text\n",
        "\n",
        "data = df['index'].tolist()  # Send to list."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxRXCXkMRgNu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re  # Regex\n",
        "\n",
        "new_data = \" \".join(data) \n",
        "\n",
        "final_data = re.sub(r'[^a-zA-Z^0-9]', ' ', new_data)  # Filter our characters\n",
        "\n",
        "\n",
        "characters = list(set(final_data))  # This set is our final set of characters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoTvd0vKSVla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create character-integer lookup.\n",
        "\n",
        "char_int = {character:integer for integer, character in enumerate(characters)}  # Assigns char to int for each integer and character in characters\n",
        "int_char = {integer:character for integer, character in enumerate(characters)}  # And int to char for each integer and character in characters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9z806j9OSxlz",
        "colab_type": "code",
        "outputId": "ba3eb400-7ab9-48ef-800c-49541088abd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Create sequences\n",
        "\n",
        "max_len = 40  # Our maximum sequence length.\n",
        "step = 5  # 5 steps\n",
        "sequences = []  # Empty list to populate with sequences.\n",
        "next_char = []  # Empty list to populate with the next character in sequence.\n",
        "\n",
        "encoded = [char_int[c] for c in final_data]  # This will encode values.\n",
        "\n",
        "# Creating sequences.\n",
        "for i in range(0, len(encoded) - max_len, step):\n",
        "  sequences.append(encoded[i: i + max_len])\n",
        "  next_char.append(encoded[i + max_len])\n",
        "print('Sequences: ', len(sequences))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequences:  1055018\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PBeg2xTTWGX",
        "colab_type": "code",
        "outputId": "7aaffa2c-c86d-498d-e6fb-06337fde559c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Now, we need to create our X and y variables.\n",
        "import numpy as np\n",
        "\n",
        "X = np.zeros((len(sequences), max_len, len(characters)), dtype=np.bool)  # Makes bool to use as X variables.\n",
        "y = np.zeros((len(sequences), len(characters)), dtype=np.bool)  # Makes bools to use as y variables.\n",
        "\n",
        "# Vectorizing sequences.\n",
        "for i, sequence in enumerate(sequences):\n",
        "  for t, char in enumerate(sequence):\n",
        "    X[i, t, char] = 1\n",
        "  y[i, next_char[i]] = 1\n",
        "\n",
        "print('X shape:', X.shape)\n",
        "print('y shape:', y.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X shape: (1055018, 40, 63)\n",
            "y shape: (1055018, 63)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EP8dhCbVCDY",
        "colab_type": "code",
        "outputId": "9881d499-d8c7-4820-f63c-cfadb4f51092",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Build our model.\n",
        "model = Sequential()  # Model instantiation.\n",
        "model.add(LSTM(128, input_shape=(max_len, len(characters))))  # LSTM layer with input shape of the maximum length of sequence, length of characters\n",
        "model.add(Dense(len(characters), activation='softmax', name='OutputLayer'))  # Dense layer (output) with the number of characters as number of nodes + softmax (because multi-class classification)\n",
        "\n",
        "model.compile(optimizer='nadam', loss='categorical_crossentropy')  # Compile our model\n",
        "model.summary()  # Let's take a look at the summary. It's good practice for making sure everything looks right!"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 128)               98304     \n",
            "_________________________________________________________________\n",
            "OutputLayer (Dense)          (None, 63)                8127      \n",
            "=================================================================\n",
            "Total params: 106,431\n",
            "Trainable params: 106,431\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPnIBxEbWF57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import sys\n",
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "\n",
        "def sample(preds):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')  # Preds as float\n",
        "    preds = np.log(preds) / 1  # Get the logarithm of preds / 1\n",
        "    exp_preds = np.exp(preds)  # Use numpy exponential method\n",
        "    preds = exp_preds / np.sum(exp_preds)  # Our new preds\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "def print_generated_text(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print()  # For readability\n",
        "\n",
        "    start_index = random.randint(0, len(final_data) - max_len - 1)  # Randomly generate where to start\n",
        "    \n",
        "    generated = ''  # Empty string\n",
        "    \n",
        "    sentence = final_data[start_index: start_index + max_len]  # This will be our seed to generate text with.\n",
        "    generated += sentence  # Add the sentence to our generated text.\n",
        "    \n",
        "    print('Generating with seed: \"' + sentence + '\"')  # Generating with seed output\n",
        "    sys.stdout.write(generated)  # This will output text. Using stdout.write due to how the output is presented; may not work with print() \n",
        "    \n",
        "\n",
        "    number_of_characters = 125  # This will be the number of characters generated. (I couldn't get this working otherwise, talk about in 1:1?)\n",
        "\n",
        "    # Here's where the magic happens!\n",
        "    for i in range(number_of_characters):\n",
        "        x_pred = np.zeros((1, max_len, len(characters)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x_pred[0, t, char_int[char]] = 1.\n",
        "            \n",
        "        preds = model.predict(x_pred, verbose=0)[0]  # Make our prediction\n",
        "        next_index = sample(preds)  # Picking the next index for our prediction.\n",
        "        next_char = int_char[next_index]  # int_char lookup for our next index.\n",
        "        \n",
        "        sentence = sentence[1:] + next_char  # Adding next character to the sentence.\n",
        "        \n",
        "        sys.stdout.write(next_char)  # Write each character.\n",
        "        sys.stdout.flush()\n",
        "    print()  # For readability\n",
        "\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=print_generated_text)  # This is our custom callback function."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57-XTbetWzgg",
        "colab_type": "code",
        "outputId": "5d2218be-82de-4c41-93ba-01f8aeb9aa26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Fit the model\n",
        "\n",
        "model.fit(X, y,\n",
        "          batch_size=32,\n",
        "          epochs=25,\n",
        "          callbacks=[print_callback])  # Usage of our callback here is what will print the text generated by the LSTM model."
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "32969/32970 [============================>.] - ETA: 0s - loss: 1.9505\n",
            "Generating with seed: \"NT SIR WALTER HERBERT SIR WILLIAM BRANDO\"\n",
            "NT SIR WALTER HERBERT SIR WILLIAM BRANDO  Ney is I have sers  for his his wirbwer lard d the wauld  and bur sitson awaty En all our ush now zame  Which seed  but in \n",
            "32970/32970 [==============================] - 174s 5ms/step - loss: 1.9505\n",
            "Epoch 2/25\n",
            "32970/32970 [==============================] - ETA: 0s - loss: 1.7049\n",
            "Generating with seed: \"KING RICHARD  Well you deserve  They wel\"\n",
            "KING RICHARD  Well you deserve  They well shy wants in the Fremio s sut  Or lears  trush  and so  a BERORIA  ney much there  My but purse  Will say  for which common\n",
            "32970/32970 [==============================] - 172s 5ms/step - loss: 1.7049\n",
            "Epoch 3/25\n",
            "32969/32970 [============================>.] - ETA: 0s - loss: 1.5874\n",
            "Generating with seed: \"evenge will come  KING  Break not your s\"\n",
            "evenge will come  KING  Break not your seen famouns quintience The lood  the pell to the muster  Thy godage your good as everton to masy ney I  he made it is his vas\n",
            "32970/32970 [==============================] - 170s 5ms/step - loss: 1.5874\n",
            "Epoch 4/25\n",
            "32964/32970 [============================>.] - ETA: 0s - loss: 1.5310\n",
            "Generating with seed: \"w torments me to rehearse  I kill d a ma\"\n",
            "w torments me to rehearse  I kill d a madishanged moon  O encle his eyes of StYoung and RICEMON  PERCIBIAND  I will ever among  were now all  The shod make aboverave\n",
            "32970/32970 [==============================] - 170s 5ms/step - loss: 1.5310\n",
            "Epoch 5/25\n",
            "32961/32970 [============================>.] - ETA: 0s - loss: 1.4964\n",
            "Generating with seed: \" reports have set the murder on  OTHELLO\"\n",
            " reports have set the murder on  OTHELLO  Yes  rise  sowal it well to one heartion narrs down    Let you often a mind EDruVhed month of paseity to any the lord  My f\n",
            "32970/32970 [==============================] - 169s 5ms/step - loss: 1.4964\n",
            "Epoch 6/25\n",
            "32970/32970 [==============================] - ETA: 0s - loss: 1.4729\n",
            "Generating with seed: \" rather hear the tabor and the pipe  I h\"\n",
            " rather hear the tabor and the pipe  I have senaly  sill  if grief when leave  as your coporuc  Nor  HEWRRCANBURD HIDRET  Now that still be owe for a relly Than vain\n",
            "32970/32970 [==============================] - 170s 5ms/step - loss: 1.4729\n",
            "Epoch 7/25\n",
            "32970/32970 [==============================] - ETA: 0s - loss: 1.4554\n",
            "Generating with seed: \"y the moon  Than such a Roman  CASSIUS  \"\n",
            "y the moon  Than such a Roman  CASSIUS  The son excell falsein d in mightly though devil  keeps me  False the lamines His daughters bear  KING EDWARD HERLUTAND  I ha\n",
            "32970/32970 [==============================] - 170s 5ms/step - loss: 1.4554\n",
            "Epoch 8/25\n",
            "32963/32970 [============================>.] - ETA: 0s - loss: 1.4418\n",
            "Generating with seed: \" past  and all that are to come  From th\"\n",
            " past  and all that are to come  From the skill in your impily  in countra and hate satring  He san glabley will for her fair em suffer  Here forbid as edre  and tha\n",
            "32970/32970 [==============================] - 167s 5ms/step - loss: 1.4418\n",
            "Epoch 9/25\n",
            "32961/32970 [============================>.] - ETA: 0s - loss: 1.4310\n",
            "Generating with seed: \"structions which begin to stop Our very \"\n",
            "structions which begin to stop Our very your company  Now by the time from their breathe it  more wordd  for steet old moon stupe  Epillet and tongue  Like a  prison\n",
            "32970/32970 [==============================] - 169s 5ms/step - loss: 1.4310\n",
            "Epoch 10/25\n",
            "32967/32970 [============================>.] - ETA: 0s - loss: 1.4220\n",
            "Generating with seed: \"  Clifford  repent in bootless penitence\"\n",
            "  Clifford  repent in bootless penitence  These thousand own with our Maria shall believes  Romemulgue her had not see the up heart  which  to much a tonguray deep m\n",
            "32970/32970 [==============================] - 169s 5ms/step - loss: 1.4220\n",
            "Epoch 11/25\n",
            "32964/32970 [============================>.] - ETA: 0s - loss: 1.4145\n",
            "Generating with seed: \"hese are not fairies  I was three or fou\"\n",
            "hese are not fairies  I was three or founce  she s artial  my lord  most  Whose of me looke but another o eremey and thos drups  I have noble Margaret o  my points o\n",
            "32970/32970 [==============================] - 169s 5ms/step - loss: 1.4145\n",
            "Epoch 12/25\n",
            "32964/32970 [============================>.] - ETA: 0s - loss: 1.4081\n",
            "Generating with seed: \"nife  No sudden mean of death  though ne\"\n",
            "nife  No sudden mean of death  though near d all warlant good  sir said strange  Restreass hichneath ert though lord  in these but honest  peace to end you having to\n",
            "32970/32970 [==============================] - 169s 5ms/step - loss: 1.4080\n",
            "Epoch 13/25\n",
            "32962/32970 [============================>.] - ETA: 0s - loss: 1.4022\n",
            "Generating with seed: \"l remain uncertain whilst  Twixt you the\"\n",
            "l remain uncertain whilst  Twixt you the cortined on hold at heir  Time to be gods he may the King  and saw you o theich dies Though carlety for a coffer d  And priv\n",
            "32970/32970 [==============================] - 170s 5ms/step - loss: 1.4022\n",
            "Epoch 14/25\n",
            "32962/32970 [============================>.] - ETA: 0s - loss: 1.3973\n",
            "Generating with seed: \"dowed with all that Adam had left him be\"\n",
            "dowed with all that Adam had left him being now  While hath spoken their swords  sure begear to enough to her scorn  MARGAREW             in lect I  when I narriculy\n",
            "32970/32970 [==============================] - 170s 5ms/step - loss: 1.3973\n",
            "Epoch 15/25\n",
            "32961/32970 [============================>.] - ETA: 0s - loss: 1.3927\n",
            "Generating with seed: \"ngling  when your diver Did hang a salt \"\n",
            "ngling  when your diver Did hang a salt  as admete  he griets  you plongery life to buting  PAROLLES  I am a fears foul in thy compone    DEMETLIO  And not my centai\n",
            "32970/32970 [==============================] - 170s 5ms/step - loss: 1.3927\n",
            "Epoch 16/25\n",
            "32966/32970 [============================>.] - ETA: 0s - loss: 1.3885\n",
            "Generating with seed: \"a stranger march Upon her gentle bosom  \"\n",
            "a stranger march Upon her gentle bosom  Thou adaments pray it presently lead them That honest who ne er he hearing in letter  Thou hast pleasus  Do we pardon a man m\n",
            "32970/32970 [==============================] - 171s 5ms/step - loss: 1.3885\n",
            "Epoch 17/25\n",
            "32967/32970 [============================>.] - ETA: 0s - loss: 1.3848\n",
            "Generating with seed: \"o tell your cousin  and then bring me wo\"\n",
            "o tell your cousin  and then bring me would be how d to find  and like wold  THESEUS  Tis not very masked to all your skines with confrued  But if I would I said is \n",
            "32970/32970 [==============================] - 172s 5ms/step - loss: 1.3848\n",
            "Epoch 18/25\n",
            "32966/32970 [============================>.] - ETA: 0s - loss: 1.3813\n",
            "Generating with seed: \"an ENGLISH SOLDIER  crying  A Talbot  A \"\n",
            "an ENGLISH SOLDIER  crying  A Talbot  A Room begon  Falsalus  BASSANIO  Harry  it less I Tood well overless  Iss after groatively  and lose to all you  and have swit\n",
            "32970/32970 [==============================] - 182s 6ms/step - loss: 1.3813\n",
            "Epoch 19/25\n",
            "32960/32970 [============================>.] - ETA: 0s - loss: 1.3788\n",
            "Generating with seed: \" as I believe you are  you must have the\"\n",
            " as I believe you are  you must have the tant to make I  Marry  my lord  dear  only knocking all  ELINCIO  Hast never from the Princes  your kniconed  Enter CELIA  l\n",
            "32970/32970 [==============================] - 175s 5ms/step - loss: 1.3789\n",
            "Epoch 20/25\n",
            "32968/32970 [============================>.] - ETA: 0s - loss: 1.3758\n",
            "Generating with seed: \"fth Harry from curb d license plucks The\"\n",
            "fth Harry from curb d license plucks The same others with fall you datce as were of minell  It is going for my titles  Cousin of Pock  Nothing with palls their palis\n",
            "32970/32970 [==============================] - 169s 5ms/step - loss: 1.3758\n",
            "Epoch 21/25\n",
            "32967/32970 [============================>.] - ETA: 0s - loss: 1.3731\n",
            "Generating with seed: \"sty bade me signify to you that he has l\"\n",
            "sty bade me signify to you that he has lengs of behold  But no carrimion more but five of the spur d  What love from tr"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: RuntimeWarning: divide by zero encountered in log\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "umpet to sit with sure  To the husband me upon\n",
            "32970/32970 [==============================] - 172s 5ms/step - loss: 1.3731\n",
            "Epoch 22/25\n",
            "32969/32970 [============================>.] - ETA: 0s - loss: 1.3708\n",
            "Generating with seed: \"man born  Master Parson  who writes hims\"\n",
            "man born  Master Parson  who writes himself  By only house of my is  if not obtwomb whom  yet s soverytapoous away  Are poor name and that parts spil  and danger off\n",
            "32970/32970 [==============================] - 171s 5ms/step - loss: 1.3708\n",
            "Epoch 23/25\n",
            "32963/32970 [============================>.] - ETA: 0s - loss: 1.3686\n",
            "Generating with seed: \"ure s wit  CELIA  Peradventure this is n\"\n",
            "ure s wit  CELIA  Peradventure this is no art their more  HORATIO  Seard such bend  God d merry colour d on thee so for a beart  And what men my in the hand bible ho\n",
            "32970/32970 [==============================] - 169s 5ms/step - loss: 1.3685\n",
            "Epoch 24/25\n",
            "32961/32970 [============================>.] - ETA: 0s - loss: 1.3666\n",
            "Generating with seed: \"leeding must be cur d   I am a Suitour  \"\n",
            "leeding must be cur d   I am a Suitour  CLEOPATRA OF PHICHILLA ULYTRA ENg  nor inflerf you keep from fix down  Winds fertlest thy both and witting da  O most a beaut\n",
            "32970/32970 [==============================] - 171s 5ms/step - loss: 1.3667\n",
            "Epoch 25/25\n",
            "32965/32970 [============================>.] - ETA: 0s - loss: 1.3644\n",
            "Generating with seed: \"y  I beseech thee  apparel thy head  And\"\n",
            "y  I beseech thee  apparel thy head  And you you and But with me the rade set Unto his mind  stwift  Enter FRETHe SALENTIUST  Your souss  may much surack  black an n\n",
            "32970/32970 [==============================] - 165s 5ms/step - loss: 1.3644\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f07765af1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zE4a4O7Bp5x1"
      },
      "source": [
        "# Resources and Stretch Goals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uT3UV3gap9H6"
      },
      "source": [
        "## Stretch goals:\n",
        "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
        "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
        "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
        "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
        "- Run on bigger, better data\n",
        "\n",
        "## Resources:\n",
        "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
        "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
        "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
        "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
        "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
      ]
    }
  ]
}