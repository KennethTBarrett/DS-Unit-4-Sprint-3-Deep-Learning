{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "U4-S3-DNN (Python 3.7)",
      "language": "python",
      "name": "u4-s3-dnn"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "LS_DS_431_RNN_and_LSTM_Assignment.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU6OSdCpKX2Z",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
        "\n",
        "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
        "\n",
        "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
        "\n",
        "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
        "\n",
        "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
        "\n",
        "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
        "\n",
        "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
        "\n",
        "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ltj1je1fp5rO",
        "colab": {}
      },
      "source": [
        "# I'm gonna import with pandas - it'll be good practice :)\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_fwf(\"https://www.gutenberg.org/files/100/100-0.txt\").reset_index()\n",
        "\n",
        "df = df.drop('Unnamed: 0', axis=1)\n",
        "df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "df = df[63:138996].reset_index(drop=True)  # The actual text\n",
        "\n",
        "data = df['index'].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxRXCXkMRgNu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "new_data = \" \".join(data)\n",
        "\n",
        "final_data = re.sub(r'[^a-zA-Z^0-9]', ' ', new_data)  # Filter our characters\n",
        "\n",
        "\n",
        "characters = list(set(final_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoTvd0vKSVla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create character-integer lookup.\n",
        "\n",
        "char_int = {character:integer for integer, character in enumerate(characters)}\n",
        "int_char = {integer:character for integer, character in enumerate(characters)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9z806j9OSxlz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "c4583095-213f-45e8-d04b-2f98c32db96e"
      },
      "source": [
        "# Create sequences\n",
        "\n",
        "max_len = 40\n",
        "step = 5\n",
        "sequences = []\n",
        "next_char = []\n",
        "\n",
        "encoded = [char_int[c] for c in final_data]\n",
        "\n",
        "for i in range(0, len(encoded) - max_len, step):\n",
        "  sequences.append(encoded[i: i + max_len])\n",
        "  next_char.append(encoded[i + max_len])\n",
        "print('Sequences: ', len(sequences))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequences:  1055018\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PBeg2xTTWGX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "f7149d15-f8d8-418f-8b83-d924fa4f01f3"
      },
      "source": [
        "# Now, we need to create our X and y variables.\n",
        "import numpy as np\n",
        "\n",
        "X = np.zeros((len(sequences), max_len, len(characters)), dtype=np.bool)\n",
        "y = np.zeros((len(sequences), len(characters)), dtype=np.bool)\n",
        "\n",
        "for i, sequence in enumerate(sequences):\n",
        "  for t, char in enumerate(sequence):\n",
        "    X[i, t, char] = 1\n",
        "  y[i, next_char[i]] = 1\n",
        "\n",
        "print('X shape:', X.shape)\n",
        "print('y shape:', y.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X shape: (1055018, 40, 63)\n",
            "y shape: (1055018, 63)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EP8dhCbVCDY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "e4efe330-c50d-4d53-ecb4-6a1bc82e0eef"
      },
      "source": [
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Build our model.\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(max_len, len(characters))))\n",
        "model.add(Dense(len(characters), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='nadam', loss='categorical_crossentropy')\n",
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 128)               98304     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 63)                8127      \n",
            "=================================================================\n",
            "Total params: 106,431\n",
            "Trainable params: 106,431\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPnIBxEbWF57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import sys\n",
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "\n",
        "def sample(preds):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / 1\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "    \n",
        "    start_index = random.randint(0, len(final_data) - max_len - 1)\n",
        "    \n",
        "    generated = ''\n",
        "    \n",
        "    sentence = final_data[start_index: start_index + max_len]\n",
        "    generated += sentence\n",
        "    \n",
        "    print('----- Generating with seed: \"' + sentence + '\"')\n",
        "    sys.stdout.write(generated)\n",
        "    \n",
        "    for i in range(400):\n",
        "        x_pred = np.zeros((1, max_len, len(characters)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x_pred[0, t, char_int[char]] = 1.\n",
        "            \n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample(preds)\n",
        "        next_char = int_char[next_index]\n",
        "        \n",
        "        sentence = sentence[1:] + next_char\n",
        "        \n",
        "        sys.stdout.write(next_char)\n",
        "        sys.stdout.flush()\n",
        "    print()\n",
        "\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57-XTbetWzgg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f65328a1-1d3a-4937-84df-e3d90e90f904"
      },
      "source": [
        "# Fit the model\n",
        "\n",
        "model.fit(X, y,\n",
        "          batch_size=32,\n",
        "          epochs=50,\n",
        "          callbacks=[print_callback])  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "32964/32970 [============================>.] - ETA: 0s - loss: 1.9518\n",
            "----- Generating text after Epoch: 0\n",
            "----- Generating with seed: \"LEAR  A plague upon you  murderers  trai\"\n",
            "LEAR  A plague upon you  murderers  trailing rongfel  lord  I tower Cathill now  She Madis and the wast  for we persing  a Calice of drey ate er walands for coptian  PAMONH  That my seblant bustor tho gnowed uphise with to be news and theme  The say Casine  My cull s diender a breesal  Heaben the nace ob dool herelf             PHe  sir  and that own forishers  Entel Entar woldinigh  if the mord  I ll ne  and when appasiens when too tri\n",
            "32970/32970 [==============================] - 176s 5ms/step - loss: 1.9518\n",
            "Epoch 2/50\n",
            "32964/32970 [============================>.] - ETA: 0s - loss: 1.6585\n",
            "----- Generating text after Epoch: 1\n",
            "----- Generating with seed: \"g foolish things  But now he throws that\"\n",
            "g foolish things  But now he throws that I walk before me  This punt d tempers  thour once so look  Duke tench the fathers  MENTOPH  Ay  my long wover  Noo your Greaise  But is a sechast they shall to facure be  thy think make nach must fair  The man staste you not of  juck men   Letwell one about you  good a boy your gathing in wold  A too  my from all your can my little are their good  O  beadume  We do was sticons You know so  what w\n",
            "32970/32970 [==============================] - 175s 5ms/step - loss: 1.6585\n",
            "Epoch 3/50\n",
            "32969/32970 [============================>.] - ETA: 0s - loss: 1.5660\n",
            "----- Generating text after Epoch: 2\n",
            "----- Generating with seed: \"w  I would he had some cause To prattle \"\n",
            "w  I would he had some cause To prattle I Solatle the survent Of cotrected thou decless  end the libe  and Vike better meet sendec  And will stread where as ond sound  And the sousing have  It ll proud and mustere me them  tekentreble hope bedun  Many all the King  Tirness spand  as never brind ear  to Now With eftermer  As I prove fly any our may  God let belice than thou shill with find ye  And we d  bet me to his cand of one of him h\n",
            "32970/32970 [==============================] - 173s 5ms/step - loss: 1.5660\n",
            "Epoch 4/50\n",
            "32964/32970 [============================>.] - ETA: 0s - loss: 1.5167\n",
            "----- Generating text after Epoch: 3\n",
            "----- Generating with seed: \"or laying on my duty  PETRUCHIO  Katheri\"\n",
            "or laying on my duty  PETRUCHIO  Katherial  Good and droves Chus  Becure among man  nor enesor s abuse restearage a khackwards  STEPHORA  Cassius  marries of There is a serve  and mariture one  NAMPES  I  that not so are with there s cases and honour and you go dain d  With worthe Soults  your fearch  Or to thUn  For  ashal and cutjome appriber d was honouring the most tenceing when I must be we grow  MRSSIMONET  whethin and orchurent d\n",
            "32970/32970 [==============================] - 172s 5ms/step - loss: 1.5167\n",
            "Epoch 5/50\n",
            "32967/32970 [============================>.] - ETA: 0s - loss: 1.4862\n",
            "----- Generating text after Epoch: 4\n",
            "----- Generating with seed: \"r eye withal  KING  If we did think His \"\n",
            "r eye withal  KING  If we did think His purits  dear duty  yet agait  A wirs in king to be to sit  Wilt thou do thy dew  be the Whise not our say mys Breast  APRONOO  I have he teling thee beany  Myself is the samp of you  CLOWN  Enter no hill we  Some in  thy gods on  CHANSIL  Daps wilday  Im nest confec d man to dear and all  You yourself  Cillain  trumpet I pray hape  brive kins  EDOBATOUS  EXETER  Which how she wak he is blazing hei\n",
            "32970/32970 [==============================] - 173s 5ms/step - loss: 1.4862\n",
            "Epoch 6/50\n",
            "32967/32970 [============================>.] - ETA: 0s - loss: 1.4636\n",
            "----- Generating text after Epoch: 5\n",
            "----- Generating with seed: \"ronicus would not relent  Therefore away\"\n",
            "ronicus would not relent  Therefore away the our your honour  have a trumen lion par of convean wither a traitor  I have doth half  BOY  If such alove is much  Marcusal toom at that shall man  Sometia  I thanks the challendest your turning in all Curnowner  SITUNIAN  Aemast the fipputery shake to battle  Was with you of going  This vale this touble trumener  MENENIUS  M son  CELIA  I may like holdins  is arm book  and preasom munder  I \n",
            "32970/32970 [==============================] - 173s 5ms/step - loss: 1.4636\n",
            "Epoch 7/50\n",
            "32965/32970 [============================>.] - ETA: 0s - loss: 1.4470\n",
            "----- Generating text after Epoch: 6\n",
            "----- Generating with seed: \"adly night  PLANTAGENET  Good Master Ver\"\n",
            "adly night  PLANTAGENET  Good Master Verwa  Danks like  and curketh of the Emperarve untell men As I could the cleasur chargenest her forefore  BURGUADY  I will  he doth these times  as I pray  HOSTESS  Come  sir  till come remouse a our shourniments  I am some unto king  True her shodoul nom  and never from the arm shall come it  Ance them Flouts to swear d  Thou make stanged my boundless  a purpe more youth  HOTSPUR  Well have welcome\n",
            "32970/32970 [==============================] - 173s 5ms/step - loss: 1.4470\n",
            "Epoch 8/50\n",
            "32970/32970 [==============================] - ETA: 0s - loss: 1.4334\n",
            "----- Generating text after Epoch: 7\n",
            "----- Generating with seed: \"t  For violent fires soon burn out thems\"\n",
            "t  For violent fires soon burn out themselves into so rua to request  I lieght you  A stry burdules  if shown all the pack d of her  O  leave  God strong in promise  I am  give me half starn of boots nor togy  and I you feat  As fair lost hath follow  PLEOPLES  Trues of him  Those than part of this lost kindens unknorles  LEONTES  Good men have I  Who have not seas of my heart to estains  They of love the TAPHIUS  I ll did disural up  I\n",
            "32970/32970 [==============================] - 172s 5ms/step - loss: 1.4334\n",
            "Epoch 9/50\n",
            "32962/32970 [============================>.] - ETA: 0s - loss: 1.4226\n",
            "----- Generating text after Epoch: 8\n",
            "----- Generating with seed: \"er other music  follows the two young Le\"\n",
            "er other music  follows the two young Lew burn that practiclet  but some very mardle sure retty dursome  Odd other dead your good preferstatifu  Enter PUCKIUS  FRENCH MORDELUS  Is we af she is good canconturation of our all accused stand PERICLES  And here you prave the years  My churcle morty It is our blown of stould you she  ANTIOMAS  How do you plast of as substen and sir  Rays her acciolian  Pleasute me gownes  can  Keigure  And th\n",
            "32970/32970 [==============================] - 173s 5ms/step - loss: 1.4226\n",
            "Epoch 10/50\n",
            "32965/32970 [============================>.] - ETA: 0s - loss: 1.4136\n",
            "----- Generating text after Epoch: 9\n",
            "----- Generating with seed: \" Lords of Ross and Willoughby  Bloody wi\"\n",
            " Lords of Ross and Willoughby  Bloody with a life  thou art so  Did Queen  Becausinable a ginding in young  MATCL  Why  and in them proud  something but a lian defell d wither fare To citing burh more In lives straine in Orent of or  for she standment  And are not preserve  that walk falle gone pate fall unsinent  KING  GLOUCESTER  My proper my sensel  The lies the monalers combating like a heart one  And death  bucking one  for excelle\n",
            "32970/32970 [==============================] - 172s 5ms/step - loss: 1.4136\n",
            "Epoch 11/50\n",
            "13643/32970 [===========>..................] - ETA: 1:35 - loss: 1.3998"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zE4a4O7Bp5x1"
      },
      "source": [
        "# Resources and Stretch Goals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uT3UV3gap9H6"
      },
      "source": [
        "## Stretch goals:\n",
        "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
        "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
        "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
        "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
        "- Run on bigger, better data\n",
        "\n",
        "## Resources:\n",
        "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
        "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
        "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
        "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
        "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
      ]
    }
  ]
}